{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.env import PACKAGE_DIR\n",
    "from src.utils.call_api.schema import EmbeddingEngineEnum\n",
    "from src.retrievers.models.bm25_retriever import KeywordRetriever\n",
    "from src.retrievers.models.vector_retriever import VectorRetriever\n",
    "from src.retrievers.models.ensemble_retriever import EnsembleRetriever\n",
    "from scripts.evaluate_retriver import evaluate\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "query_df = pd.read_csv(PACKAGE_DIR/'data/raw/valid_sets/query_ans_txt.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定量評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = [250, 300, 350, 400, 450, 500]\n",
    "overlap = [0.2, 0.3, 0.4, 0.5]\n",
    "comb_list = list(itertools.product(chunksize, overlap))\n",
    "\n",
    "vec_norm_result = {'chunksize': [], 'overlap': [], 'top5': [], 'top10': [], 'top25': [], 'top50': []}\n",
    "vec_aug_result = {'chunksize': [], 'overlap': [], 'top5': [], 'top10': [], 'top25': [], 'top50': []}\n",
    "key_norm_result = {'chunksize': [], 'overlap': [], 'top5': [], 'top10': [], 'top25': [], 'top50': []}\n",
    "key_aug_result = {'chunksize': [], 'overlap': [], 'top5': [], 'top10': [], 'top25': [], 'top50': []}\n",
    "\n",
    "for chunksize, overlap in comb_list:\n",
    "    index_dir = PACKAGE_DIR.joinpath(f'data/database/valid_sets/chartext_chunk{chunksize}_lap{overlap}')\n",
    "    base_chunked_df = pd.read_csv(index_dir/'openai_large.csv')\n",
    "    vec_norm_result['chunksize'].append(chunksize)\n",
    "    vec_aug_result['chunksize'].append(chunksize)\n",
    "    key_norm_result['chunksize'].append(chunksize)\n",
    "    key_aug_result['chunksize'].append(chunksize)\n",
    "\n",
    "    vec_norm_result['overlap'].append(overlap)\n",
    "    vec_aug_result['overlap'].append(overlap)\n",
    "    key_norm_result['overlap'].append(overlap)\n",
    "    key_aug_result['overlap'].append(overlap)\n",
    "\n",
    "\n",
    "    # normal index検索\n",
    "    normal_index = base_chunked_df.drop_duplicates(subset='text')\n",
    "    vecretriever = VectorRetriever(normal_index, model=EmbeddingEngineEnum.Large)\n",
    "    keyretriever = KeywordRetriever(normal_index)\n",
    "\n",
    "    # augmented index検索\n",
    "    aug_vecretriever = VectorRetriever(base_chunked_df, emb_column_name='augmented_embedding', target_column_name='augmented_text', model=EmbeddingEngineEnum.Large)\n",
    "    aug_keyretriever = KeywordRetriever(base_chunked_df, tokenized_column_name='augmented_tokenized_text', target_column_name='augmented_text')\n",
    "\n",
    "    for topk in [5, 10, 25, 50]:\n",
    "        vec_norm_result[f'top{topk}'].append(evaluate(vecretriever, topk, query_df))\n",
    "        vec_aug_result[f'top{topk}'].append(evaluate(aug_vecretriever, topk, query_df, evidence_column='text', require_columns=['chunk_id', 'text']))\n",
    "        key_norm_result[f'top{topk}'].append(evaluate(keyretriever, topk, query_df))\n",
    "        key_aug_result[f'top{topk}'].append(evaluate(aug_keyretriever, topk, query_df, evidence_column='text', require_columns=['chunk_id', 'text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定性評価"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 400\n",
    "overlap = 0.3\n",
    "index_df = pd.read_csv(PACKAGE_DIR.joinpath(f'data/database/valid_sets/chartext_chunk{chunksize}_lap{overlap}/openai_large.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_index = index_df.drop_duplicates(subset='text')\n",
    "keyretriever = KeywordRetriever(normal_index)\n",
    "vecretriever = VectorRetriever(normal_index, model=EmbeddingEngineEnum.Large)\n",
    "aug_keyretriever = KeywordRetriever(index_df, tokenized_column_name='augmented_tokenized_text', target_column_name='augmented_text')\n",
    "aug_vecretriever = VectorRetriever(index_df, emb_column_name='augmented_embedding', target_column_name='augmented_text', model=EmbeddingEngineEnum.Large)\n",
    "\n",
    "# ens retriever\n",
    "ensretriever = EnsembleRetriever(vecretriever, keyretriever)\n",
    "aug_ensretriever = EnsembleRetriever(aug_vecretriever, aug_keyretriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(vecretriever, 5, query_df, return_miss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(aug_vecretriever, 5, query_df, return_miss=True, require_columns=['text', 'chunk_id'], evidence_column='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    score = evaluate(ensretriever, 10, query_df, weights=[vec, 1-vec], ensemble_method='rrf', rank_impact_mitigator=0)\n",
    "    print(f'score is {score} when vec: {vec}, key: {1-vec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    score = evaluate(aug_ensretriever, 10, query_df, weights=[vec, 1-vec], ensemble_method='rrf', rank_impact_mitigator=0, require_columns=['text', 'chunk_id'], evidence_column='text')\n",
    "    print(f'score is {score} when vec: {vec}, key: {1-vec}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_id = 0\n",
    "print(query_df.loc[q_id, ['problem', 'evidence']])\n",
    "aug_ensretriever.retrieve(query_df.loc[q_id, 'problem'], top_k=10, weights=[vec, 1-vec], ensemble_method='rrf', rank_impact_mitigator=0, require_columns=['text', 'chunk_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecretriever.retrieve('主人公の吉田の患部は主にどこですか', top_k=10, require_columns=['chunk_id', 'text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-1-meYwZ3LN-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
